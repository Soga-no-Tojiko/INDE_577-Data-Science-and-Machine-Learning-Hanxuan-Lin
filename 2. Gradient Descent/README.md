# Gradient Descent

In mathematics, gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.

# Datasets

In this part, we generate a function that combines different sine and cosine functions. This function has multiple local minima and local maxima that take different values.

# Aim

In this try we will conduct gradient descent method from scratch and test if it will work on a arbitrarily generated dataset. And this will also show if simple gradient descent method could help find a global minima within a selected interval.
